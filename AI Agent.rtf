{\rtf1\ansi\ansicpg1252\cocoartf2867
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue-Bold;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab560
\pard\pardeftab560\pardirnatural\partightenfactor0

\f0\b\fs40 \cf0 AI agent\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0

\f1\b0\fs26 \cf0 \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97local\'97\'97\'97\'97\'97\'97\'97\
from fastapi import FastAPI\
from pydantic import BaseModel\
import requests\
\
app = FastAPI()\
\
class PromptRequest(BaseModel):\
    prompt: str\
\
@app.post("/generate")\
def generate(req: PromptRequest):\
    response = requests.post(\
        "http://localhost:11434/api/generate",\
        json=\{\
            "model": "llama3.2:latest",\
            "prompt": req.prompt,\
            "stream": False\
        \}\
    )\
\
    return response.json()\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
"""\
Single-file Inference Server using:\
- FastAPI  \uc0\u8594  HTTP API\
- Groq     \uc0\u8594  Hosted LLM inference\
- dotenv   \uc0\u8594  Secure API key management\
"""\
\
# ---------------------------\
# 1. Imports\
# ---------------------------\
\
from fastapi import FastAPI\
from pydantic import BaseModel\
from groq import Groq\
from dotenv import load_dotenv\
import os\
\
\
# ---------------------------\
# 2. Load environment variables\
# ---------------------------\
\
# Loads variables from .env into environment\
load_dotenv()\
\
GROQ_API_KEY = os.getenv("GROQ_API_KEY")\
\
if not GROQ_API_KEY:\
    raise RuntimeError("GROQ_API_KEY not found. Check your .env file.")\
\
\
# ---------------------------\
# 3. App & client initialization\
# ---------------------------\
\
# Create FastAPI application\
app = FastAPI(\
    title="Simple Inference Server",\
    description="FastAPI wrapper over Groq LLM API",\
    version="1.0.0"\
)\
\
# Create Groq client (connects to Groq's inference backend)\
groq_client = Groq(api_key=GROQ_API_KEY)\
\
# Model configuration (easy to change later)\
MODEL_NAME = "llama-3.1-8b-instant"\
MAX_TOKENS = 200\
TEMPERATURE = 0.7\
\
\
# ---------------------------\
# 4. Request schema\
# ---------------------------\
\
class PromptRequest(BaseModel):\
    """\
    Expected request body:\
    \{\
        "prompt": "your input text"\
    \}\
    """\
    prompt: str\
\
\
# ---------------------------\
# 5. Inference endpoint\
# ---------------------------\
\
@app.post("/generate")\
def generate_text(req: PromptRequest):\
    """\
    Receives a prompt, sends it to Groq for inference,\
    and returns the generated text.\
    """\
\
    completion = groq_client.chat.completions.create(\
        model=MODEL_NAME,\
        messages=[\
            \{"role": "user", "content": req.prompt\}\
        ],\
        temperature=TEMPERATURE,\
        max_tokens=MAX_TOKENS\
    )\
\
    # Extract model response\
    output_text = completion.choices[0].message.content\
\
    return \{\
        "prompt": req.prompt,\
        "response": output_text\
    \}\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
.env file \
\
GROQ_API_KEY = "API key from groq\'94\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
pip install fastapi uvicorn groq python-dotenv\
}